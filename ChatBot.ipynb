{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8uYj1tbQsrWQbA/oJ3tKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkaransail/DataScience/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9xP3jh6RQP8",
        "outputId": "eaf8d4ff-b00b-43dc-ad4b-5c391a94f7cf"
      },
      "source": [
        "!pip install pyttsx3 #used for speech output\n",
        "!pip install SpeechRecognition #used for speech input\n",
        "!pip install pyspellchecker #spelling checker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyttsx3\n",
            "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: pyttsx3\n",
            "Successfully installed pyttsx3-2.90\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 35 kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7gBCaCtRrJj"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giTI7ITWRqdz",
        "outputId": "f4a00013-6986-48d1-8982-147a9728e670"
      },
      "source": [
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "import urllib\n",
        "import bs4 as bs\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import string \n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pandas import DataFrame\n",
        "import pyttsx3 \n",
        "import speech_recognition as sr\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr0a5_S5SU-7"
      },
      "source": [
        "**Data or Information Collection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw1uwvIHSqS2"
      },
      "source": [
        "page1=requests.get('https://www.timeanddate.com/weather/india') #switch on the Internet option from right side window"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLVeJAQ8S8vR"
      },
      "source": [
        "def temp(topic):\n",
        "    \n",
        "    page = page1\n",
        "    soup = BeautifulSoup(page.content,'html.parser')\n",
        "\n",
        "    data = soup.find(class_ = 'zebra fw tb-wt zebra va-m')\n",
        "\n",
        "    tags = data('a')\n",
        "    city = [tag.contents[0] for tag in tags]\n",
        "    tags2 = data.find_all(class_ = 'rbi')\n",
        "    temp = [tag.contents[0] for tag in tags2]\n",
        "\n",
        "    indian_weather = pd.DataFrame(\n",
        "    {\n",
        "        'City':city,\n",
        "        'Temperature':temp\n",
        "    }\n",
        "    )\n",
        "    \n",
        "    df = indian_weather[indian_weather['City'].str.contains(topic.title())] \n",
        "    \n",
        "    return (df['Temperature'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vSZqDggTNeq"
      },
      "source": [
        "**Scrape wiki for city details**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXOu7hOzTUIc"
      },
      "source": [
        "def wiki_data(topic):\n",
        "    \n",
        "    topic=topic.title()\n",
        "    topic=topic.replace(' ', '_',1)\n",
        "    url1=\"https://en.wikipedia.org/wiki/\"\n",
        "    url=url1+topic\n",
        "\n",
        "    source = urllib.request.urlopen(url).read()\n",
        "\n",
        "    # Parsing the data/ creating BeautifulSoup object\n",
        "    soup = bs.BeautifulSoup(source,'lxml')\n",
        "\n",
        "    # Fetching the data\n",
        "    text = \"\"\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        text += paragraph.text\n",
        "\n",
        "    import re\n",
        "    # Preprocessing the data\n",
        "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
        "    text = re.sub(r'\\s+',' ',text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d',' ',text)\n",
        "    text = re.sub(r'\\s+',' ',text)\n",
        "    \n",
        "    \n",
        "    return (text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plhNdbM0TrPx"
      },
      "source": [
        "**Text Cleaning**\n",
        "\n",
        "Now comes the most important part of nlp i.e. text cleaning. Without this we can't get useful results.\n",
        "\n",
        "Removing special char\n",
        "Stemming\n",
        "Lemmatization\n",
        "Stop words\n",
        "Part of speech (POS)\n",
        "Name entity recognition (NER)\n",
        "Sentiment Analysis\n",
        "Spelling checker\n",
        "Tokenization\n",
        "Stemming\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxbuouo2Ubd4"
      },
      "source": [
        "**Removing special characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jVnMuHKVUiVg",
        "outputId": "d633e6b1-1653-4bc8-a370-d6dd9df9c4eb"
      },
      "source": [
        "def rem_special(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    return(text.translate(remove_punct_dict))\n",
        "\n",
        "sample_text=\"I am sorry! I don't understand you.\"\n",
        "rem_special(sample_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am sorry I dont understand you'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpzC2OMQUqDU"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s7hENJx5UwiK",
        "outputId": "d91618a6-b353-40ad-da4c-ddf394a265a7"
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "def stemmer(text):\n",
        "    words = word_tokenize(text) \n",
        "    for w in words:\n",
        "        text=text.replace(w,PorterStemmer().stem(w))\n",
        "    return text\n",
        "\n",
        "stemmer(\"He is Eating. He played yesterday. He will be going tomorrow.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'He is eat. He play yesterday. He will be go tomorrow.'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Ukc-pWU4KW"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EitRpWU7U-BW",
        "outputId": "82b9d492-864b-42fa-ae52-a61de35d6af1"
      },
      "source": [
        "lemmer = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "sample_text=\"rocks corpora better\" #default noun\n",
        "LemTokens(nltk.word_tokenize(sample_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rock', 'corpus', 'better']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnv_VfCVGx6"
      },
      "source": [
        "**Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-KtsUdYKVNCs",
        "outputId": "b096901e-3e68-4ccc-a271-4ad81dd903b3"
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"This is a sample sentence, showing off the stop words filtration.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sample sentence , showing stop words filtration .'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XiCa51NVYLo"
      },
      "source": [
        "**Part of Speech (POS) or Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8OGsuicVj7Q"
      },
      "source": [
        "import spacy \n",
        "spacy_df=[]\n",
        "spacy_df1=[]\n",
        "df_spacy_nltk=pd.DataFrame()\n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "  \n",
        "# Process whole documents \n",
        "sample_text = (\"The heavens are above. The moral code of conduct is above the civil code of conduct\") \n",
        "doc = nlp(sample_text) \n",
        "  \n",
        "# Token and Tag \n",
        "for token in doc:\n",
        "    spacy_df.append(token.pos_)\n",
        "    spacy_df1.append(token)\n",
        "\n",
        "\n",
        "df_spacy_nltk['origional']=spacy_df1\n",
        "df_spacy_nltk['spacy']=spacy_df\n",
        "#df_spacy_nltk"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ8NjKmhVzXh"
      },
      "source": [
        "**Name Entity Recognition (NER)\n",
        "spaCy supports the following entity types:\n",
        "\n",
        "PERSON\n",
        "\n",
        "NORP (nationalities, religious and political groups)\n",
        "\n",
        "FAC (buildings, airports etc.)\n",
        "\n",
        "ORG (organizations)\n",
        "\n",
        "GPE (countries, cities etc.)\n",
        "\n",
        "LOC (mountain ranges, water bodies etc.)\n",
        "\n",
        "PRODUCT (products)\n",
        "\n",
        "EVENT (event names)\n",
        "\n",
        "WORK_OF_ART (books, song titles)\n",
        "\n",
        "LAW (legal document titles)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uOrEAvhV3Dx",
        "outputId": "1ee3f5c7-4320-4cc6-8e94-a9713b3e0779"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "\n",
        "def ner(sentence):\n",
        "    doc = nlp(sentence) \n",
        "    for ent in doc.ents: \n",
        "        print(ent.text, ent.label_) \n",
        "    \n",
        "\n",
        "sentence = \"A gangster family epic set in 1919 Birmingham, England; centered on a gang who sew razor blades in the peaks of their caps, and their fierce boss Tommy Shelby.\"\n",
        "ner(sentence)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1919 DATE\n",
            "Birmingham GPE\n",
            "England GPE\n",
            "Tommy Shelby PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qN5GQezWCC4"
      },
      "source": [
        "**Sentiment Analysis\n",
        "The sentiment returns polarity. The polarity score is a float within the range [-1.0, 1.0]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb1YvylcWJeX",
        "outputId": "1433b5ee-2396-4f6e-8072-8757f95ca609"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def senti(text):\n",
        "    testimonial = TextBlob(text)\n",
        "    return(testimonial.polarity)\n",
        "\n",
        "sample_text=\"This apple is good\"\n",
        "print(\"polarity\",senti(sample_text))\n",
        "sample_text=\"This apple is not good\"\n",
        "print(\"polarity\",senti(sample_text))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "polarity 0.7\n",
            "polarity -0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJOPU5cNWTBE"
      },
      "source": [
        "**Spelling checker**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rsfmdFotWVl7",
        "outputId": "b109b933-1d1c-40bf-ad22-9d85940a4926"
      },
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "def spelling(text):\n",
        "    splits = sample_text.split()\n",
        "    for split in splits:\n",
        "        text=text.replace(split,spell.correction(split))\n",
        "        \n",
        "    return (text)\n",
        "    \n",
        "    \n",
        "sample_text=\"hapenning elephnt texte luckno sweeto\"\n",
        "spelling(sample_text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'happenning elephant text lucknow sweet'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYBjy0hUWOg5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG6KG9IIWb2g"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AKgaw02WkYl",
        "outputId": "b9bb7a26-8d24-40cf-8055-24ad76db750e"
      },
      "source": [
        "print(nltk.sent_tokenize(\"Hey how are you? I am fine.\"))\n",
        "print(nltk.word_tokenize(\"Hey how are you? I am fine.\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hey how are you?', 'I am fine.']\n",
            "['Hey', 'how', 'are', 'you', '?', 'I', 'am', 'fine', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtzRVPSEWuo3"
      },
      "source": [
        "**Creating dictionary for cities\n",
        "For chatbot the user may input city name in many form. For example \"bangalore\" can be referred as \"bangalore\", \"bengaluru\" or \"blr\". The chatbot shoud be able to identify to which city the user is referring to.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBGUGGxRWwuh"
      },
      "source": [
        "city = {} \n",
        "city[\"bangalore\"]=[\"bangalore\",\"bengaluru\",\"blr\"]\n",
        "city[\"lucknow\"] = [\"lucknow\", \"lko\"]\n",
        "city[\"delhi\"]=[\"new delhi\",\"ndls\",\"delhi\"]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o5GpsczcW25a",
        "outputId": "8115e227-fd9d-4072-9c2b-e26a7dc7c19b"
      },
      "source": [
        "def city_name(sentence):\n",
        "    for word in sentence.split():\n",
        "        for key, values in city.items():\n",
        "            \n",
        "            if word.lower() in values:\n",
        "                return(key)\n",
        "                \n",
        "    \n",
        "city_name(\"blr\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bangalore'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmambNHwW9mi"
      },
      "source": [
        "**Word Embedding**\n",
        "\n",
        "\n",
        "Word embedding is the representation of word so that it can be feed as input the machine learning models. ML model take input only in form of numerical values, so words are converted to vector form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "oD4PKkBzXENh",
        "outputId": "8083c3a1-f8fc-486a-86de-bb3252505ede"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documentA = 'This is about Messi'\n",
        "documentB = 'This is about TFIDF'\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>about</th>\n",
              "      <th>is</th>\n",
              "      <th>messi</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.448321</td>\n",
              "      <td>0.448321</td>\n",
              "      <td>0.630099</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.448321</td>\n",
              "      <td>0.448321</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.630099</td>\n",
              "      <td>0.448321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      about        is     messi     tfidf      this\n",
              "0  0.448321  0.448321  0.630099  0.000000  0.448321\n",
              "1  0.448321  0.448321  0.000000  0.630099  0.448321"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bCNg3zuXNaa"
      },
      "source": [
        "**Generating answer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvhJFTPXQVs"
      },
      "source": [
        "def LemNormalize(text):\n",
        "    text=rem_special(text) #remove special char\n",
        "    text=text.lower() # lower case\n",
        "    text=remove_stopwords(text) # remove stop words\n",
        "    \n",
        "    return LemTokens(nltk.word_tokenize(text))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hthRq60YXYJH"
      },
      "source": [
        "**Cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb8y_3k4XUJR"
      },
      "source": [
        "#Generating answer\n",
        "def response(user_input):\n",
        "    \n",
        "    ToGu_response=''\n",
        "    sent_tokens.append(user_input)\n",
        "    \n",
        "    \n",
        "    \n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')   \n",
        "    all_word_vectors = word_vectorizer.fit_transform(sent_tokens)  \n",
        "    \n",
        "   \n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors) \n",
        "    idx=similar_vector_values.argsort()[0][-2]\n",
        "    \n",
        "\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "    \n",
        "    if(vector_matched==0):\n",
        "        ToGu_response=ToGu_response+\"I am sorry! I don't understand you.\"\n",
        "        return ToGu_response\n",
        "    else:\n",
        "        ToGu_response = ToGu_response+sent_tokens[idx]\n",
        "        return ToGu_response"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6EYXgH4Xrdz"
      },
      "source": [
        "**Get city name**\n",
        "Take input from user. Then fetch wiki data and weather details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajw9acHFXu-5"
      },
      "source": [
        "#topic=str(input(\"Please enter the city name you want to ask queries for: \")) #Uncomment this line to take input from command prompt or jupyter notebook.\n",
        "topic=\"bangalore\" # sample city\n",
        "\n",
        "topic=city_name(topic) # fetch city name in case of invalid input or any discrepancy in the city name\n",
        "\n",
        "text=wiki_data(topic) # fetch wiki data about city\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(text)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(text)# converts to list of words\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1KWssNUY9qK"
      },
      "source": [
        "**Greetings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a_hSN7GXAAv"
      },
      "source": [
        "# greetings Keyword matching\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfFSaeXJZasY"
      },
      "source": [
        "**Places**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2TEwdIHZYvD"
      },
      "source": [
        "PLACES_INPUTS = (\"places\", \"monuments\", \"buildings\",\"places\", \"monument\", \"building\")\n",
        "\n",
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "\n",
        "def ner(sentence):\n",
        "    places_imp=\"\"\n",
        "    doc = nlp(sentence) \n",
        "    for ent in doc.ents: \n",
        "        if (ent.label_==\"FAC\"):\n",
        "            #print(ent.text, ent.label_) \n",
        "            places_imp=places_imp+ent.text+\",\"+\" \"\n",
        "            \n",
        "    return(places_imp)\n",
        "    \n",
        "\n",
        "places_imp=ner(text) \n",
        "\n",
        "\n",
        "s=places_imp\n",
        "l = s.split() \n",
        "k = [] \n",
        "for i in l: \n",
        "  \n",
        "    # If condition is used to store unique string  \n",
        "    # in another list 'k'  \n",
        "    if (s.count(i)>1 and (i not in k)or s.count(i)==1): \n",
        "        k.append(i) \n",
        "\n",
        "PLACES_RESPONSES = ' '.join(k)\n",
        "\n",
        "def places(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in PLACES_INPUTS:\n",
        "            return (PLACES_RESPONSES)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZmdwzjZuv8"
      },
      "source": [
        "**Chat**\n",
        "\n",
        "This is the main chat function. Sentiment score is for finding the sentiment of the user input. According to user sentiment we can reply to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_ET81rSZz1Y",
        "outputId": "03e890d3-788a-48d7-b58e-f78299448aaf"
      },
      "source": [
        "continue_dialogue=True\n",
        "print(\"ToGu: Hello, I am Jarvis\")\n",
        "#speak(\"Hello\")\n",
        "\n",
        "while(continue_dialogue==True):\n",
        "    user_input = input(\"User:\")\n",
        "    user_input=user_input.lower()\n",
        "    user_input=spelling(user_input) #spelling check\n",
        "    print(\"Sentiment score=\",senti(user_input)) #sentiment score\n",
        "    \n",
        "    if(user_input!='bye'):\n",
        "        if(user_input=='thanks' or user_input=='thank you' ):\n",
        "            print(\"ToGu: You are welcome..\")\n",
        "            #speak(\" You are welcome\")\n",
        "            \n",
        "        else:\n",
        "            if(greeting(user_input)!=None):\n",
        "                tmp=greeting(user_input)\n",
        "                print(\"ToGu: \"+tmp)\n",
        "                #speak(tmp)\n",
        "                \n",
        "          \n",
        "                \n",
        "            elif(places(user_input)!=None):\n",
        "                tmp=places(user_input)\n",
        "                print(\"ToGu: Important places are \"+tmp)\n",
        "                #speak(\"Important places are\")\n",
        "                #speak(tmp)\n",
        "                \n",
        "            else:\n",
        "                print(\"ToGu: \",end=\"\")\n",
        "                temp=response(user_input)\n",
        "                print(temp) \n",
        "                #speak(temp)\n",
        "                sent_tokens.remove(user_input)\n",
        "                \n",
        "\n",
        "    else:\n",
        "        continue_dialogue=False\n",
        "        print(\"ToGu: Goodbye.\")\n",
        "        #speak(\"goodbye\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ToGu: Hello, I am Jarvis\n",
            "User:hey jarvis\n",
            "Sentiment score= 0.0\n",
            "ToGu: hello\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}